{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Week 3, Python Exercise \n",
    "\n",
    "In this exercise we implement a simple environment and an agent which takes random actions. The goals of this exercise are: <br>\n",
    "* To get familiar with the Python language\n",
    "* To implement explicitly some basic elements of an RL problem\n",
    "* To get used to the basic vocabulary (most RL literature is in english)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specification of the task\n",
    "\n",
    "### Define the Rewards and Transitions of the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list [] to store the reward we get at each state\n",
    "# https://www.w3schools.com/python/python_lists.asp\n",
    "rewards = [0, -1, 0, -10, 4, 3, 7, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward of state s4:\n",
      "4\n",
      "At state 5 I obtain a reward r = 3\n",
      "\n",
      "Use a for-loop to iterate over all indices:\n",
      "At state s0 you obtain a reward of 0\n",
      "At state s1 you obtain a reward of -1\n",
      "At state s2 you obtain a reward of 0\n",
      "At state s3 you obtain a reward of -10\n",
      "At state s4 you obtain a reward of 4\n",
      "At state s5 you obtain a reward of 3\n",
      "At state s6 you obtain a reward of 7\n",
      "At state s7 you obtain a reward of 5\n"
     ]
    }
   ],
   "source": [
    "# Access by (zero-based) index:\n",
    "print('reward of state s4:')\n",
    "r = rewards[4]\n",
    "print(r)\n",
    "\n",
    "# store the stateID in a variable. Use .format() for string-interpolation\n",
    "stateID = 5\n",
    "r = rewards[stateID]\n",
    "print('At state {} I obtain a reward r = {}'.format(stateID, r))\n",
    "\n",
    "print('\\nUse a for-loop to iterate over all indices:')\n",
    "for i in range(len(rewards)):\n",
    "    # In Python, indentation is important! All indented lines belong to the body of the for loop\n",
    "    print('At state s{} you obtain a reward of {}'.format(i, rewards[i]))\n",
    "    # this line is still inside the loop\n",
    "# this line is outside the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[[1, 2], [3, 4], [5, 6], [-1, -1], [7, 7], [-1, -1], [-1, -1], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# The transition matrix T is used to store the following information:\n",
    "# \"For each state s, and action a, what is the next state?\"\n",
    "\n",
    "# Not all states have a next_state. Some states are TERMINAL. Depending on context, such\n",
    "# states are sometimes also called goal states.\n",
    "# For each state we store whether it's terminal or not.\n",
    "\n",
    "is_terminal = [False, False, False, True, False, True, True, True]\n",
    "\n",
    "\n",
    "print(is_terminal[5])\n",
    "\n",
    "# We have 8 states (s0 to s7) and 2 actions (a0 and a1). We can store the \n",
    "# transitions in a 7-by-2 matrix.\n",
    "# In python we can use a list of lists. Later we will use numpy, which provides matrices.\n",
    "T = [ \n",
    "    [1, 2],  # in state s0, taking action a0 brings us to state s1, a1 to s2\n",
    "    [3,4],   # in state s1, taking action a0 brings us to state s3, a1 to s4\n",
    "    [5,6],   # in state s2, taking action a0 brings us to state s5, a1 to s6\n",
    "    [-1, -1], # state s3 is a terminal state. next-state is undefined (we use -1 here).\n",
    "    [7, 7],   # from state s4, both actions lead to state s7\n",
    "    [-1, -1], # state s5 is a terminal state\n",
    "    [-1, -1], # state s6 is a terminal state\n",
    "    [-1, -1]  # state s7 is a terminal state\n",
    "]\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# example of how to use the transition matrix T\n",
    "current_state = 2\n",
    "action = 0\n",
    "next_state = T[current_state][action]\n",
    "print(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T(0,0) -> 1\n",
      "T(0,1) -> 2\n",
      "T(1,0) -> 3\n",
      "T(1,1) -> 4\n",
      "T(2,0) -> 5\n",
      "T(2,1) -> 6\n",
      "T(3,0) -> -1\n",
      "T(3,1) -> -1\n",
      "T(4,0) -> 7\n",
      "T(4,1) -> 7\n",
      "T(5,0) -> -1\n",
      "T(5,1) -> -1\n",
      "T(6,0) -> -1\n",
      "T(6,1) -> -1\n",
      "T(7,0) -> -1\n",
      "T(7,1) -> -1\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all states and actions.\n",
    "# Note two things:\n",
    "# 1. range(8) iterates from 0 to 7 . 8 is not included\n",
    "# 2. note how nested arrays are accessed/indexed. \n",
    "#   Later, when using numpy, this is done differently\n",
    "for s in range(8): \n",
    "    for a in range (2):\n",
    "        next_state = T[s][a] \n",
    "        print('T({},{}) -> {}'.format(s, a, next_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the policy of the agent\n",
    "\n",
    "We have now defined the environment. Next, we define an agent, that is, we have to define a function that selects actions. In this simple example, our agent selects randomly between the two possibilities with 50% probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5279799407556023\n",
      "0.7994987328162075\n",
      "124\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# Python is organized in modules. There are thousands of modules with a plethora of functions. \n",
    "# Before we can use a function from the random module, we have to import it:\n",
    "# https://www.w3schools.com/python/module_random.asp\n",
    "import random\n",
    "# just a few test calls:\n",
    "print(random.random())\n",
    "print(random.random())\n",
    "print(random.randint(-800,1000))\n",
    "print(random.randint(-800,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I took action a1 and moved to state S2. My reward for doing this is 0.\n"
     ]
    }
   ],
   "source": [
    "# the agent starts at state s0.\n",
    "current_state = 0\n",
    "# the sum of rewards R (also called the Return) is initialized with 0\n",
    "R = 0\n",
    "\n",
    "# In our environment, one can select from two actions, a0 and a1. Our agent follows a random policy where \n",
    "# each action is take with equal probability:\n",
    "action = random.randint(0,1)\n",
    "\n",
    "# observe the next state and the reward\n",
    "next_state = T[current_state][action]\n",
    "r = rewards[next_state] # use lower case r for the immediate reward. Upper case R is the sum of rewards, called Return.\n",
    "R += r\n",
    "\n",
    "print('I took action a{} and moved to state S{}. My reward for doing this is {}.'.format(action, next_state, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "* make sure you understand the code examples.\n",
    "* run the previous code block a few times and observe how the agent takes random actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "An **episode** is a sequence of states and actions from a start state to a goal state (=terminal state). Our goal is to estimate the **Return R** an agent can expect to collect in one full episode.<br>\n",
    "\n",
    "* Copy the code example given above\n",
    "* Wrap it inside a loop, such that the agent runs a full episode\n",
    "* Run this loop a few times and observe the total reward **R** the agent collects in each episode. Do you get reasonable results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I took action a0 and moved to state S1. My reward for doing this is -1.\n",
      "I took action a1 and moved to state S4. My reward for doing this is 4.\n",
      "I took action a1 and moved to state S7. My reward for doing this is 5.\n",
      "My actions gave me a total reward R of 8\n"
     ]
    }
   ],
   "source": [
    "# the agent starts at state s0.\n",
    "current_state = 0\n",
    "# the sum of rewards R (also called the Return) is initialized with 0\n",
    "R = 0\n",
    "\n",
    "while (not is_terminal[current_state]):\n",
    "    # In our environment, one can select from two actions, a0 and a1. Our agent follows a random policy where \n",
    "    # each action is take with equal probability:\n",
    "    action = random.randint(0,1)\n",
    "\n",
    "    # observe the next state and the reward\n",
    "    next_state = T[current_state][action]\n",
    "    r = rewards[next_state] # use lower case r for the immediate reward. Upper case R is the sum of rewards, called Return.\n",
    "    R += r\n",
    "\n",
    "    print('I took action a{} and moved to state S{}. My reward for doing this is {}.'.format(action, next_state, r))\n",
    "    current_state = next_state\n",
    "\n",
    "print('My actions gave me a total reward R of {}'.format(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 3\n",
    "* Wrap the code from exercise 2 inside a loop which completes 10'000 episodes. Each episode starts at state S0 and terminates at one of the terminal states.\n",
    "* Calculate the mean of the total reward **R**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards R measured: 10000\n",
      "Mean of total reward: 1.883\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "# list of saved rewards R\n",
    "got_rewards = []\n",
    "\n",
    "for x in range(10000):\n",
    "    # the agent starts at state s0.\n",
    "    current_state = 0\n",
    "    # the sum of rewards R (also called the Return) is initialized with 0\n",
    "    R = 0\n",
    "    while (not is_terminal[current_state]):\n",
    "        # In our environment, one can select from two actions, a0 and a1. Our agent follows a random policy where \n",
    "        # each action is take with equal probability:\n",
    "        action = random.randint(0,1)\n",
    "\n",
    "        # observe the next state and the reward\n",
    "        next_state = T[current_state][action]\n",
    "        r = rewards[next_state] # use lower case r for the immediate reward. Upper case R is the sum of rewards, called Return.\n",
    "        R += r\n",
    "\n",
    "        # print('I took action a{} and moved to state S{}. My reward for doing this is {}.'.format(action, next_state, r))\n",
    "        current_state = next_state\n",
    "\n",
    "    got_rewards.append(R)\n",
    "\n",
    "\n",
    "print('Total rewards R measured: {}'.format(len(got_rewards)))\n",
    "print('Mean of total reward: {}'.format(mean(got_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 4 \n",
    "In the previous exercises, the agent took actions with equal probabilities: $\\pi(s, left) = \\pi(s, right) = 0.5 $.\n",
    "* Change this policy to $\\pi(s, left) = 0.3 , \\pi(s, right) = 0.7 $. (Hint: use this function: https://www.w3schools.com/python/ref_random_random.asp and go 'left' if the random number is <0.3 .\n",
    "* Rerun the previous experiment with this new policy and estimate the expected total reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of a total of 10000 rewards is 4.6684\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "# list of saved rewards R\n",
    "got_rewards = []\n",
    "\n",
    "for x in range(10000):\n",
    "    # the agent starts at state s0.\n",
    "    current_state = 0\n",
    "    # the sum of rewards R (also called the Return) is initialized with 0\n",
    "    R = 0\n",
    "    while (not is_terminal[current_state]):\n",
    "        # In our environment, one can select from two actions, a0 and a1. Our agent follows a random policy where \n",
    "        # each action is take with the according probability:\n",
    "        if random.random() < 0.3:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "\n",
    "        # observe the next state and the reward\n",
    "        next_state = T[current_state][action]\n",
    "        r = rewards[next_state] # use lower case r for the immediate reward. Upper case R is the sum of rewards, called Return.\n",
    "        R += r\n",
    "\n",
    "        # print('I took action a{} and moved to state S{}. My reward for doing this is {}.'.format(action, next_state, r))\n",
    "        current_state = next_state\n",
    "\n",
    "    got_rewards.append(R)\n",
    "\n",
    "print('The mean of a total of {} rewards is {}'.format(len(got_rewards), mean(got_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
